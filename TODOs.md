- [x] [P0] Write some scaffolding for the Mochi API so that it can read and generate new spaced repetition cards and put them in my account right away
- [x] [P0] Write prompt to generate spaced repetition cards.
- [x] [P0] Scrape data from website so that tool can read data from the sources.
  - [x] Semianalysis articles (semianalysis_scraper.py)
  - [x] Random tech blogs (generic_tech_blog_scraper.py - hybrid approach with domain-specific extractors)
- [x] Use LLM (Claude or Qwen3) to generate spaced repetition cards.
  - [x] [P0] First have the tool generate the output.
  - [x] [P0] Optimize LLM inference on Mac M4 (takes way too long right now; one run at ~20 mins; impossible to test)
  - [x] [P0] After the data is verified, confirm that it can generate the spaced repetition cards in your Mochi account to the appropriate deck
- [x] [P0] Build E2E script so that it can be executed in one CLI cmd
- [x] [P0] Verify that the spaced repetition cards are actually useful and testing you on knowledge that you want to retain!
- [ ] Gradually expand sources of data to all your sources of consumption:
  - [x] [P0] Tech blogs
  - [x] [P0] YT videos (yt-dlp)
  - [ ] [P0] Books (EPUB downloads)
    - [ ] Books can be huge with a ton of tokens, so have to build some context splitting functionality (and parallelize it with GPU server like vLLM for lower latency) to get this to work with model's limited context window.
  - [ ] [P0] Podcasts
  - [ ] [P1] News articles with required auth / strong bot protections (e.g. Medium, WSJ, Financial Times, )
    - **[Sample Script to Scrape WSJ Articles](https://github.com/philippe-heitzmann/WSJ_WebScraping_NLP/blob/master/scraping/scrape.py)**
    - Also explore options by scraping articles from WaybackMachine, archive.is, archive.ph, etc
- [ ] [P1] Make AI tools platform-agnostic; get it working on GPUs and Macs using a simple CLI flag from E2E script: spaced_repetition_card_gen_pipeline.py
  - [ ] Whisper ASR model that transcribe YT videos
    - [ ] Nvidia GPU
    - [x] Apple Silicon
  - [ ] LLMs that generate spaced repetition cards
    - [ ] Nvidia GPU
    - [x] Apple Silicon
- [ ] [P1] Productize this tool to see if anyone would use it
  - [ ] Deploy on web server as API
  - [ ] Build a web app interface (Next.JS with Tailwind)
  - [ ] Build a chrome/firefox extension
  - [ ] Look into increasing performance of tool (latency first, then throughput)
    - [ ] Run load tests
    - [ ] Write web server logic in Rust or Go
    - [ ] Deploy card generation on vLLM or SGLang or llama.cpp servers to increase throughput (and possibly latency) of service
      - [ ] Deploy on GPU workers for faster token generation
    *NOTE: Not expecting this tool to achieve significant scale or anything like that; this is purely out of curiosity and fun*
- [ ] [P2] See if multimodal support can be added so that we can create richer cards that ask and answer questions about image data as well
- [ ] [P2] Extend this tool to support content Q&A (talk-to-article) with citations
  - [ ] Extend sources to an entire website domain (e.g. resolve.ai)
    - [ ] Must be able to traverse (BFS, DFS) all the pages in the domain
  - Example Usecase: "Based on all the knowledge in resolve.ai, how would i build a knowledge graph for an agent to root-cause performance and capacity regressions?"
    - *Basically, consume an entire startup website's knowledge and use AI reasoning to answer a sophisticated question about how/why a startup accomplishes something.*
  - *NOTE: This should probably be a separate application. Let's keep this tool simple.*
